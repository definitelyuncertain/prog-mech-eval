# Program Embeddings for Rapid Mechanism Evaluation

This repository contains code for an as yet unpublished paper.

```
Narayanaswami, S.K, Fridovich-Keil, D., Chaudhuri, S., Stone, P.
Program Embeddings for Rapid Mechanism Evaluation
```

## Abstract
Mechanisms such as auctions, voting and traffic control systems incentivize agents in multiagent systems in order to achieve or optimize certain global outcomes such as safety, productivity, and welfare. Designing mechanisms in the form of programs would bring several benefits such as interpretability, transparency and verifiability. However, program synthesis for finding suitable mechanisms from a search space represented by programs leads to computationally expensive two-level optimization problems, where mechanisms need to be evaluated in an inner loop, which entails learning agent responses for each mechanism. Multi-Task Learning (MTL) approaches efficiently learn parameterized agent strategies that can be used to act in various tasks, here mechanisms defined by programs. Such multi-task agent policies allow for rapid evaluation of mechanism performance for a given program by bypassing the need to learn agent responses from scratch, thus allowing for efficient evaluation of mechanisms. In this work, we use learned program representations (or _embeddings_) to provide suitable task contexts that make MTL feasible with combinatorially large programmatic search spaces of mechanisms. We demonstrate experimentally that program embeddings generated by an off-the-shelf Code2Vec model are sufficient for reconstructing matrix games based on only a programmatic description. The embeddings are also able to serve as task context to guide a set of agent strategies to act near-optimally in mechanisms from the search space.

## Overview

We investigate whether agents can learn to act in
multiagent systems (or mechanisms) based on only a
programmatic description of their environment. For this,
we use deep program representations to
convert source code into vector embeddings. The
embeddings are then given as input to a Multi-Task policy,
which learns to use the information in the embedding to
act optimally. This framework is illustrated in the
following image:

![](schematic.png "An illustration of agent learning and acting in the proposed framework. The program representing the mechanism (here defining the operation of a traffic signal at an intersection) is encoded by a neural network (trained separately using standard techniques) into a vector embedding. Agents learn and act with task-conditioned policies that take in both observations from the environment as well as the program embedding as task context. When evaluating a new mechanism after learning, the agent policies can be executed in the same way without the need for learning a separate set of policies.")

### Experiments

Our experiments are performed on a domain of [two player, zero-sum matrix games](https://en.wikipedia.org/wiki/Normal-form_game)
whose entries are 0 or 1. The payoff matrices for
these games are generated by programs from a simple
Domain Specific (programming) Language, or DSL. The
DSL consists of sequences of statements of the form `set(row min, row max, col min, col max, value);`. For example, the program
```
set(7, 9, 0, 15, 1);
set(0, 15, 7, 9, 1);
```
produces the matrix shown below (white areas have
value one):
![](mat_cross.png "Matrix for the above program")

The matrices shown here depict player 1's cost when
player 1 chooses a particular row and player 2
chooses a particular column. Player 1 aims to 
minimize this cost while player 2 aims to maximize it.

We use a [Code2Vec](https://github.com/tech-srl/code2vec) model as the embedding
network without further training (i.e **off-the shelf.**).

### Matrix Prediction
A model is trained to reconstruct the game matrix
based on only a program such as the above as input,
in the form of its embedding (or _code vector_).
Here are some example reconstructions along with the
ground truth matrices.

![](viz_mat_pred.png "Matrix prediction results visualized for a set of randomly held-out test programs. The top row shows the matrices generated by the programs (which are not given as inputs to the network), and the bottom row visualizes the predicted matrices. The reconstructions closely resemble the ground truth matrices.")

### Nash Strategy Prediction

A set of multi-task policies for each agent is trained to
imitate Nash equilibrium strategies using
Behavioral Cloning, based on _only_ the code vector
of the program generating the matrix game. Ground
truth equilibrium strategies are computed using
standard Linear Programming approaches implemented
in the [PATH](http://pages.cs.wisc.edu/~ferris/path.html) solver through [TensorGames.jl](https://github.com/forrestlaine/TensorGames.jl).
Below are shown some game matrices and the policy
probabilities for each agent (hover over the image for more details).

![](viz_nash_pred.png "Learned policy probabilities for a set of random test programs: The top row shows the matrix generated by the programs (note that these are not given as inputs to the network). The middle row visualizes the policy probabilities for player 1. The intensity of the color of each horizontal line represents the  probability of player 1 choosing that row of the matrix. The probabilities are normalized so that the largest value for any particular strategy is 1 (darkest). Similarly, the bottom row visualizes the policy probabilities for player 1 for choosing columns of the matrix. We see that both agents have learned to act near-optimally by avoiding rows/columns where the other agent can always win (whenever possible). They do so even when such blocks are very small, such as the rightmost matrix or the third from the right, where player 1 is able to choose." )

The above results demonstrate how program representations
can encode the information necessary to _describe 
and act optimally_ in multiagent environments.
Further, _general-purpose_ models
such as Code2Vec can provide useful task contexts 
even when used _off-the-shelf_ without further training.


## Codebase Installation

A separate Anaconda environment is recommended for
all the installation steps.

### Packages

* First, install the requirements for Code2Vec outlined in `code2vec/README.md`.
* Julia (tested with 1.8.0).
* Python packages: `pip install -r requirements.txt`.
* Install ANTLR4 and its Python runtime (see [here](https://www.antlr.org/download.html)).
* Jupyter for visualization notebooks.

### Code2Vec Model

Download the Code2Vec model:
```
wget https://s3.amazonaws.com/code2vec/model/java14m_model.tar.gz
tar -xvzf java14m_model.tar.gz
```

Extract it into `../data`, and ensure that the model
is present at
```
../data/java14m_model/models/java14_model/saved_model_iter8.release
```

### ANTLR4 Parser

Then run the following assuming that you have installed ANTLR4 into `../antlr4`.

```
export CLASSPATH="`pwd`/../antlr4/antlr-4.11.1-complete.jar:$CLASSPATH"
alias antlr4="java -jar `pwd`/../antlr4/antlr-4.11.1-complete.jar"
alias grun='java org.antlr.v4.gui.TestRig'
```

Next
```
cd antlr
antlr4 -Dlanguage=Python3 matrix.g4 -visitor -o matrix_grammar
```

**Note** : The first set of commands needs to be run
for every session/terminal window. You can also add
it to your `.bashrc`. The second set needs to be run
the first time and every time you modify the grammar
in `matrix.g4`.


## Training

To train models for the experiments described in the paper, run

```
python -m experiments.runner --config <path to config.yaml>
```

The training configs for the paper are available in:
* `configs/nash_prediction.yaml` : Learning to predict Nash strategies from the Code Vector.
* `configs/matrix_prediction.yaml` : Learning to reconstruct the game matrix from the Code Vector.

### Dataset Caching

Since it can take a long time to compute the Code Vectors
for a large dataset of programs, you might wish to
pre-compute those, as well as the Nash solutions for
the corresponding matrix games and save them to disk.

For this, use the following script:
```
python -m experiments.create_dataset --load ../data/java14m_model/models/java14_model/saved_model_iter8.release --export_code_vectors
```

This step is optional, and needs to be specified in
the training configuration. Otherwise, a new set of
training programs will be randomly generated.


# Visualizations

To generate the plots shown above, use the Jupyter notebooks `viz_nash_prediction.ipynb` and `viz_reconstruction.ipynb`.